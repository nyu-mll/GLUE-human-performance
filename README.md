# Human Performance on GLUE

This data collection project is designed to get a good estimate of human performance on the General Language Understanding Evaluation (GLUE) tasks. The tasks covered by this data collection effort are,

+ Acceptability judgement with The Corpus of Linguistic Acceptability (CoLA)
+ Sentiment classification with Stanford Sentiment Treebank (SST)
+ Paraphrase equivalence with Microsoft Research Paraphrase Corpus (MRPC)
+ Sentiment classification with Semantic Textual Similarity Benchmark
+ Question equivalence with Quora Question Pairs
+ Natural language inference, or textual entailment, with Multi-genre Natrual Language Inference (MNLI), matched and mismatched datasets.
+ Question answering with Question NLI (an adaptation of SQuAD)
+ Natural language inference, or textual entailment, with Recognizing Textual Entailment
+ Commonsense reasoning with Winograd NLI (adaptated from Winograd schema)

[//]: # (To-Do: Add links to all papers and datasets!)

# FAQ & Instructions: NYU Labeling task on Hybrid